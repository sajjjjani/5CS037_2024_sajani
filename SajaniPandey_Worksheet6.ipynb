{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1q7qa35fzJ5R4oAptuPq99hmod-akMX8d","authorship_tag":"ABX9TyNQzgP5kvuBlm1/bYuRo15d"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"AkL10mXAMEvS","executionInfo":{"status":"ok","timestamp":1736046338205,"user_tz":-345,"elapsed":882,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}}},"outputs":[],"source":["#3.1. Implementation from scratch step-by-step guide\n","#3.1.1. Step1:Data understanding, analysis, and preparation\n","#To-do-1:\n","#1. Read and observe the dataset\n","import pandas as pd\n","df = pd.read_csv(\"/content/drive/MyDrive/AIWeek2/student.csv\")"]},{"cell_type":"code","source":["#2. Print top(5) and bottom(5) of the dataset\n","print(\"Top 5 rows of the dataset:\")\n","print(df.head(5))\n","print(\"Bottom 5 rows of the dataset:\")\n","print(df.tail(5))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hhhVTHwDbOaT","executionInfo":{"status":"ok","timestamp":1736046410853,"user_tz":-345,"elapsed":726,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"668da56e-7349-4afe-c446-ac281bd3d5d5"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 5 rows of the dataset:\n","   Math  Reading  Writing\n","0    48       68       63\n","1    62       81       72\n","2    79       80       78\n","3    76       83       79\n","4    59       64       62\n","Bottom 5 rows of the dataset:\n","     Math  Reading  Writing\n","995    72       74       70\n","996    73       86       90\n","997    89       87       94\n","998    83       82       78\n","999    66       66       72\n"]}]},{"cell_type":"code","source":["#3. Print the information of datasets\n","print(df.info())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f-gw-nRybolF","executionInfo":{"status":"ok","timestamp":1736046477589,"user_tz":-345,"elapsed":395,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"d7ddac7e-4439-4484-9496-9bf5902ef567"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1000 entries, 0 to 999\n","Data columns (total 3 columns):\n"," #   Column   Non-Null Count  Dtype\n","---  ------   --------------  -----\n"," 0   Math     1000 non-null   int64\n"," 1   Reading  1000 non-null   int64\n"," 2   Writing  1000 non-null   int64\n","dtypes: int64(3)\n","memory usage: 23.6 KB\n","None\n"]}]},{"cell_type":"code","source":["#4. Gather the descriptive info about the dataset\n","print(df.describe())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hus7yHMLbw4C","executionInfo":{"status":"ok","timestamp":1736046516088,"user_tz":-345,"elapsed":399,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"0f0d3676-ab21-47c4-bf53-d1ac7b9764c0"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["              Math      Reading      Writing\n","count  1000.000000  1000.000000  1000.000000\n","mean     67.290000    69.872000    68.616000\n","std      15.085008    14.657027    15.241287\n","min      13.000000    19.000000    14.000000\n","25%      58.000000    60.750000    58.000000\n","50%      68.000000    70.000000    69.500000\n","75%      78.000000    81.000000    79.000000\n","max     100.000000   100.000000   100.000000\n"]}]},{"cell_type":"code","source":["#5. Split your data into feature(x) and label(y)]\n","x = df[['Math', 'Reading']]\n","y = df['Writing']\n","print(\"Features(x) shape: \", x.shape)\n","print(\"Labels(y) shape: \", y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7pLO7W9Zb5o2","executionInfo":{"status":"ok","timestamp":1736048262824,"user_tz":-345,"elapsed":403,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"cbcc7ae5-3fb2-47e9-f479-3230f1d84941"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Features(x) shape:  (1000, 2)\n","Labels(y) shape:  (1000,)\n"]}]},{"cell_type":"code","source":["#To-do-2:\n","import numpy as np\n","# Preparing feature matrix x\n","x = x.values.T\n","#x.values: converts dataframe into numpy array\n","print(\"Feature matrix(x): \")\n","print(x)\n","print(\"Shape of x: \", x.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_tHrDkY5cW84","executionInfo":{"status":"ok","timestamp":1736048321442,"user_tz":-345,"elapsed":447,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"4987828b-f16f-4948-eca6-30d1dd6248dc"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature matrix(x): \n","[[48 62 79 ... 89 83 66]\n"," [68 81 80 ... 87 82 66]]\n","Shape of x:  (2, 1000)\n"]}]},{"cell_type":"code","source":["# Preparing label vector y\n","y = y.values\n","print(\"Label vector(y): \")\n","print(y)\n","print(\"Shape of y: \", y.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fy7nLTFIf4hw","executionInfo":{"status":"ok","timestamp":1736048269664,"user_tz":-345,"elapsed":447,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"f3dceeac-3a28-4434-aa9e-fcf2c68ae2cc"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Label vector(y): \n","[ 63  72  78  79  62  85  83  41  80  77  64  90  45  77  70  46  76  44\n","  85  72  53  66  75  49  84  83  68  66  77  78  74  83  72  65  46  66\n","  50  79  68  46  86  70  61  53  72  75  50  77 100  81 100  87  78  48\n","  50  44  48  43  67  78  58  91  92  78  42  85  73  83  61  58  60  55\n","  48  62  68  59  62  48  74  63  80  79  73  79  45  67  89  77  81  88\n","  53  68  79  77  63  73  60  67 100  79  26  51  80  57  41  78  68  49\n","  76  41  71  77  89  86  55  80  56  74  85  80  73  74  86  56  53  44\n","  41  59  71  81  74  78  67  53  56  75  82  79  99  76  59  96  75  61\n","  56  88  65 100  79  55  61  83  74  59  54  47  82  74  59  74  84  59\n","  43  65  61  78  84  73  73  92  63  72  61  59  70  87  78  65  73  62\n","  69  55  73  63  67  86  78  85  83  80  60  90  56  70  55  80  82  60\n","  78  76  94  75  68  71  85  46  58  46  84  58  57  59  77  63  68  99\n","  48  91  57  80  46  75  59  87  82  79  66  68  66  61  66  63  72  73\n","  77  84  83  42  72  76  76  39  74  43  63  74  52  31  65  45  87  63\n","  51  82  86  76  27  70  79  66  61  62  47  17  65  76  75  66  59  61\n","  93  40  66  43  71  64  55  86  65  70  65  53  49  67  76  95  76  48\n","  60  53  69  78  62  66  51  52  46  42  77  57 100  84  68  48  72  50\n","  72  55  72  77  56  94  67  82  75  80  60  73  74  62  53  69  75  60\n","  58  71  87  74  87  73  78  76  74  55  94  71  76  59  91  57  83  59\n","  93  64  58  79  96  76  64  70  80  33  95  64  92  34  72  81  57  79\n","  84  82  54  45  54  62  49  74  59  63  83  62  72  72  65  65  54  78\n","  82  85  74  83  71  83  77  66  75  52  68  84  67  70  41  91  46  58\n","  67  70  83  64 100  49  77  57  67  80  74  41  67  59  86  88  57  80\n","  58  52  31  84  97  71  62  58  71  41  66 100  51  35  81  94  72  38\n","  82  79  55  75  90  95  65  39  85  86  54  93  69  84  78  58  73  60\n","  44  67  69  55  59  88  42  78  84  68  66  51  43  38  69  90  73  67\n","  57  81  63  80  78  65  74  80  60  60  63  64  72  51  71  63  82  76\n","  39  79  48  70  90  73  58 100  80  75  72  79  52  56  65  45  59  61\n","  47  62  83  90  76  72  69  57  56  40  79  48  57  47  78  45  74  69\n","  59  85  45  54  72  74  75  55  49  53  83  22 100  67  83  46  43  74\n","  64  35  67  87  77  91  74  96  82  78  73  52  91  66  67  71  74  71\n","  61  47  76  85  93  41  81  86  53  91  68  96  48  71  75  72  71  62\n","  67  53  74  63  82  57  69  52  91  73  73  75  36  71  62 100  50  74\n","  60  75  83  83 100  67  71  77  67  95  52  71  74  60  67  79  75  95\n","  69  80  48  61  82  39  70  70  69  32  79  53  59  83 100  80  80  82\n","  56  83  85  88  81  95  63  70  89  59  56  62  95  63  82  69  58  74\n","  66  82  94  70  78  63  91  70  62  79  65  74  56  65 100  70  66  54\n","  72  90  56  65  50  95  38  76  84  76  55  85  70  73  80  83  53  67\n"," 100  67  44  96  48  77 100  40  91  55  41  25  63  59  63  77  46  49\n","  46  93  39  58  87  57  77 100  65  34  87  81  63  69  74  70  93  63\n","  81  81  63  87  76  54  89  63  76  79  75  50  36  82  83  85  82  41\n","  82  45  57  88  81  98  61  95  84  71  52  71  90  75  62  63  86  70\n","  77  68  80  67  67  89  60  79  80  78  70  72  43  14  54  92  71  65\n","  58  56  67  64  81  55  45  86  52  75  81  62  42  21  72  55  66  69\n","  86  67  78  85  66  47 100  63  62  61  69  57  76  52  47  51  61  45\n","  59  81  65  53  61  90  74  62  67  50  84  70  52  92  65  65  67  72\n","  66  62  99  62  53  57  78  56  87  79  63  87  86  75  70  60  49  41\n","  78  58  75  89  34  60  80  85  73  58  69  74  52  58  79  86  61  68\n","  67  48  65  73  57  73  57  80  85  81  61  69 100  99  92  72  57  44\n","  59  62  93  64  57  72  40  85  60  83  63  74  44  61  74  68  78  50\n","  70  68  82  46  96 100  44  41  95  79  67  52  87  75  61  42  60  57\n","  64  52  68  58  93  75  77  66  63  90  43  65  95  86  31  95  52  63\n","  87  70  59  84  79  77  75  66  69  85  63  50  58  80  47  55  61  87\n","  77  54  66  68  54  69  74  81  72  61  76  63  64  73  62  92  69  70\n","  65  53  74  61  80  85  62  80  83  56  76  52  51  74  57  63  61  87\n","  60  54  89  67  56  70  90  94  78  72]\n","Shape of y:  (1000,)\n"]}]},{"cell_type":"code","source":["# Define the weight matrix W\n","d = x.shape[0] #number of features\n","W = np.random.rand(d,1) #creates a matrix of dx1 and initializes the values randomly from 0 to 1(exclusive)\n","# np.random.rand(): generates random numbers from 0 to 1(exclusive)\n","print(\"Weight Matrix(W):\")\n","print(W)\n","print(\"Shape of W: \", W.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qgnNbIHcf6Gn","executionInfo":{"status":"ok","timestamp":1736048326662,"user_tz":-345,"elapsed":453,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"0c2949dc-24c3-42ba-e33f-eadb104ad055"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Weight Matrix(W):\n","[[0.4391633 ]\n"," [0.99534029]]\n","Shape of W:  (2, 1)\n"]}]},{"cell_type":"code","source":["# Compute y = WT.X\n","y_pred = np.dot(W.T, x)\n","print(\"Predicted values(y_pred): \")\n","print(y_pred)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LZrH-h9Zg4St","executionInfo":{"status":"ok","timestamp":1736048337105,"user_tz":-345,"elapsed":458,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"ee170433-510f-4281-eace-7fceacb60ee0"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted values(y_pred): \n","[[ 88.7629781  107.85068808 114.32112396 115.98965491  89.61241329\n","  113.91085207 114.35001538  67.97784575 104.42550392 115.40458652\n","   95.90660464 119.20970313  68.38811764 113.20877     96.46278162\n","   65.31397452  97.01895861  67.53868245 120.7612204  107.58776931\n","   86.7145147   90.72476726 101.08844202  79.42498307 115.46236934\n","  119.44373049  92.92058378  99.24366654 113.6479333  109.72435498\n","  107.82179667 119.88289379 101.76163268 102.05489089  67.5979133\n","   81.35643279  74.7112004  109.57844989 103.9574492   62.23983139\n","  118.65352615 110.8959398   90.25671254  71.16900256  94.47210105\n","  101.96676863  75.15036371 101.76163268 139.05872604  99.12665286\n","  143.45035908 120.64420672 107.85068808  70.52470331  70.40768963\n","   72.04732917  81.97184062  71.28601624 102.4940542  111.36399451\n","   81.09351401 127.58269732 141.45967851 114.2330017   70.02775718\n","  129.77851384  92.27628453 124.77292099  89.58352188  86.7145147\n","   90.57886217  84.63571186  70.61282557  89.05623631  91.04691688\n","   72.95454719  93.91592407  59.9558926  112.09641603  94.99938662\n","  117.62929445 108.72901469  96.69680898 107.82179667  62.03469544\n","   94.00404633 118.21436284 110.69080385 109.37331394 114.55515132\n","   74.47717304 103.84043552  98.01429889 105.2460477   91.16393056\n","   93.3886385   83.23009968  90.81288953 136.42374622 124.47966278\n","   45.34793792  75.15036371 105.50896647  69.79228179  57.70229326\n","  110.69080385 110.66191244  64.43564791 110.13462687  63.58621271\n","   95.14529171 111.48100819 126.00228863 123.5132139   66.66035584\n","  113.47168877  77.25805796 105.94812978 125.56312533 111.10107574\n","   98.21943484 114.32112396 116.10666859 100.26934627  66.77736952\n","   68.85617236  52.43233361  79.01471117 104.89500666 123.89459438\n","  109.46143621 105.65487156  95.78959096  79.89303778  81.53267732\n","  113.88196066 119.91323323 118.2735937  123.39620022 123.13328145\n","   89.84644065 134.98924263 100.53226504  97.66325786  91.28094424\n","  113.14953914 102.58217646 139.05872604 111.7742664   82.35177307\n","   90.13969886 119.2978254  100.53226504  83.9625212   87.94388234\n","   67.09951914 122.07871031 103.07912259  84.89863063 103.84043552\n","  114.672165    90.19748169  58.25847024 100.79518381 101.73274127\n","  105.59708874 127.75894185 102.63995929  94.2669651  138.47365765\n","   82.00073204 105.50896647  95.32153624  83.87439893  99.56581616\n","  124.94771749 121.49364192  91.57420245 104.42550392  87.97277376\n","   97.75138012  84.51869818  96.75603983  90.37372622  92.07114858\n","  113.35467509 107.93881035 113.3835665  119.26893398 124.12862174\n","   84.07953488 132.50161593  91.57420245 105.01202034  72.74941124\n","  105.39195279 126.55846562  97.54624418 103.31314995 110.22274913\n","  126.6754793   99.65393843  89.61241329 101.82086354 123.80647211\n","   61.94657318  86.39236507  68.65103641 105.71410242  87.21146083\n","   85.28001111  90.34483481 111.10107574  75.94056805  95.23341398\n","  138.47365765  63.35218535 125.62090816  87.18256942 121.81579154\n","   79.54199674 104.83577581  94.32619596 113.20877    112.03718518\n","  108.93415064  90.95879462 103.40127222  89.29026367 110.54489876\n","   74.06690115  89.37838593 111.65725273 102.43482334 110.48566791\n","  120.52719304 115.87264123  56.85285806  97.89728521 116.51694048\n","  103.19613627  50.85192493  96.69680898  71.49115218  90.16859028\n","  111.48100819  71.05198888  46.02112859  90.37372622  59.60485157\n","  130.8908678   90.69587585  72.51538388 114.90619236 117.77519954\n","  110.36865423  54.42301419 102.43482334 105.71410242  91.69121613\n","   92.3644068   91.80822981  76.4389622   27.25556823  91.80822981\n","  125.76826128 105.53785788  96.5797953  104.60174845  85.10376658\n","  139.46899793  67.53868245  96.11174059  58.72652496 115.63861387\n","   86.94854206  84.63571186 116.51694048 101.82086354  96.22875426\n","  101.84975495  71.10977171  70.96386661  95.32153624 105.39195279\n","  121.72766927 105.27493911  70.58393416  74.85710549  86.97743347\n","  100.82552325 108.70012328  95.5555636  100.73740098  78.78068382\n","   78.34152051  69.96852633  62.88413064 111.89128008  85.95320177\n","  132.64752102 129.66150016  91.25205283  67.30465509  96.22875426\n","   86.91965064  97.4292305   65.8701515  112.56447075 121.05447861\n","   79.13172485 125.3868808   83.113086   120.29316568 108.70012328\n","  106.2702794  103.5182859  110.13462687  94.14995142  95.43854992\n","   76.5848673   94.44320964 100.85441466 103.37238081  94.88237294\n","  107.70478299 120.70343757  96.37465936 115.17055916 117.27825341\n","  115.22834198  99.97608805  91.48608019  75.94056805 138.59067133\n","   87.32847451 101.84975495  91.45718877 128.02186062  78.13638457\n","  116.95610379  84.31356223 134.60931018  93.94481548  81.35643279\n","  114.40924623 137.39019509  97.89728521  92.68655642 111.3351031\n","  104.22036797  48.86124435 131.24190884  93.44786935 128.66615987\n","   48.33395878 107.90991894 118.41949879  85.19188884 107.61666072\n","  116.22368227 122.89925409  79.21984712  64.96293348  86.27535139\n","   92.33551538  67.97784575 107.03159233  96.11174059  99.85907437\n","  125.24097571  97.31221682  97.66325786 111.3351031  101.73274127\n","   83.23009968  90.16859028 103.28425854 116.86798152 125.32909797\n","  122.81113183 120.35239654  98.77561182 112.79849811 105.80222468\n","   96.31687653 100.7662924   76.93590833  97.34110823 120.05913833\n","   87.50471904 101.99566004  65.43098819 118.62463473  80.65435071\n","   78.25339825 109.78358583 102.43482334 119.20970313  94.79425067\n","  139.93705265  90.60775358 104.95278949  87.41659677  98.19054343\n","  114.87730095 104.95278949  71.81330181  91.83712123  89.37838593\n","  130.65684044 135.48763679  92.04225717 113.00363405  93.77001897\n","   82.85016723  58.37548392 114.32112396 135.0773649  103.84043552\n","   86.33313422  85.77695724 110.8959398   70.9349752   96.08284917\n","  137.50720877  69.88040406  56.1796674  109.95838234 126.6754793\n","  107.67589158  58.14145656 110.7500347  116.83909011  76.49674503\n","  103.63529958 122.16683258 131.24190884 100.85441466  57.70229326\n","  122.28384626 118.18547143  79.86414637 134.52118791 100.88330607\n","  113.88196066 120.14726059  77.93124862 102.26002684  87.27069168\n","   77.22916655  97.75138012 103.72342184  85.19188884  65.98716518\n","  118.41949879  64.23051196 123.60133617 102.75697297 102.90432609\n","   89.05623631  67.24542424  74.12613201  71.3741385   95.76069955\n","  113.70571613  98.77561182 111.21808942  91.04691688 123.27918654\n","   99.06887003 111.10107574 110.16351828  93.09682831 108.90525922\n","  115.78451897  95.76069955  81.18163628  89.81754924  76.81889466\n","  116.63395416  84.19654855 103.16724486 100.85441466 104.9816809\n","  113.20877     68.41700906 113.00363405  74.91633635 105.80222468\n","  115.31646425  99.56581616  84.95786148 136.30673254 122.69411815\n","  103.89821835  98.42457078 119.76588011  75.35549965  78.45853419\n","   97.78027153  56.15077599  85.51403847  88.52895074  82.32288166\n","   96.43389021 100.97142834 121.08337003 110.69080385 104.83577581\n","  103.69453043  80.65435071  78.69256155  66.45521989 112.21342971\n","   58.9316609   89.58352188  64.34752564 117.74630813  78.66367014\n","  115.87264123  96.34576794  88.91033122 109.34442253  71.19789397\n","   88.14901829 110.01761319 110.54489876 102.17190457  79.65901042\n","   71.81330181  85.8650795  109.75324639  41.92275376 141.25454256\n","   96.90194493 120.20504342  78.13638457  66.98250546 118.50762106\n","   88.58818159  61.68365441  92.77467869 120.29316568 112.21342971\n","  121.55142474 116.48804907 134.55007933 116.86798152 112.79849811\n","  102.08378231  80.12706514 131.65218073  93.27162482  98.10242116\n","   88.93922263 108.26095997 105.2460477   92.13037944  66.3093148\n","   98.77561182 113.20877    133.05779291  62.21093998 111.89128008\n","  116.9849952   84.8408478  122.13794117  92.13037944 132.91043979\n","   72.63239756  93.35974708 100.70850957 104.27959883 105.94812978\n","   91.39795792  91.39795792  81.649691    97.66325786 100.50337362\n","  119.32671681  87.50471904  99.62504702  79.57088816 118.41949879\n","  101.73274127  94.82314209 108.5831096   61.56664073  97.10708087\n","   86.74340611 143.45035908  80.18629599  98.95185635  88.29492338\n","   89.72942697 118.06845775 128.66615987 141.45967851  97.63436644\n","  100.00497947 116.66284557 106.24138799 133.52584763  89.49539961\n","   97.57513559  96.22875426  88.73408669  90.72476726 102.05489089\n","  112.44745707 133.52584763 103.5182859  118.56685191  75.79466296\n","  103.92855779 116.75096784  54.27710909 101.644619    97.01895861\n","  100.09310173  49.41742134 107.7336744   77.34618023  97.45812191\n","  112.00829376 138.06338576 114.76028727 114.2330017  116.45770963\n","   72.31024794 117.30714482 118.41949879 132.99856206 126.76360156\n","  140.46433822 100.29823768 106.35840167 119.7369887   92.13037944\n","   76.02869031  90.69587585 136.51186849  88.52895074 120.23538286\n","   97.34110823  95.99472691 108.17283771  98.21943484 117.51228077\n","  125.26986712 101.73274127 122.02092749  84.07953488 134.43306565\n","  111.65725273  81.03428316 117.16123973  91.01802547 112.24232112\n","   89.0273449   89.72942697 140.37621596  99.97608805 107.58776931\n","   68.53402273  92.86135293 131.09600375  85.71917441  88.73408669\n","   77.31728881 137.74123613  55.94564004 122.81113183 112.44745707\n","   87.7387464   85.63105215 104.16258515  96.02361832 108.5831096\n","  122.34307711 116.10666859  76.02869031  97.25298596 139.49788935\n","  101.17656429  73.39371049 130.30579941  68.29999538 109.6073413\n","  143.45035908  58.28736165 132.06245262  76.84778607  65.40209678\n","   36.71202497  90.16859028  81.56156873  90.60775358 110.0465046\n","   73.92099606  68.32888679  56.85285806 131.53516705  54.42301419\n","   85.4259162  116.89687293  84.86973922 109.69546356 140.5813519\n","   90.0515766   54.21787824 130.30579941 116.9849952   83.99141261\n","  102.08378231 112.76960669 103.05023118 119.09268945  95.1164003\n","  112.21342971 127.31977855  86.18722913 117.74630813 122.0498189\n","   83.55224931 124.85959523  90.60775358 103.86932694 102.40593193\n","  107.17749742  81.53267732  61.2444911  120.87823408 118.97567577\n","  126.03118005 103.86932694  55.27244938 104.74765354  80.53733703\n","   92.01336576 114.43813764 118.74164841 140.37621596  90.37372622\n","  140.46433822 119.26893398 100.06421032  87.35881395 110.54489876\n","  130.33469082  97.57513559 100.50337362  87.97277376 118.82977068\n","  104.27959883 103.07912259  97.22409455 118.50762106  93.53599162\n","   94.5313319  122.40085994  75.50140474 103.43016363 105.42084421\n","  104.86466722  97.54624418 102.87398665  73.27669681  24.6205884\n","   84.86973922 129.89552752 107.90991894  98.98074777  91.01802547\n","   84.45946733 103.07912259  90.57886217 113.20877    103.72342184\n","   69.0613083  124.71369014  85.30890252 109.66657215 115.90153265\n","   93.09682831  67.09951914  42.82997178 109.66657215  80.88837807\n","   93.2427334  106.35840167 127.31977855  94.91126435 115.75562755\n","  122.46009079  94.70612841  74.47717304 143.45035908  92.13037944\n","   83.34711336  91.01802547 106.56353761  79.68790184 115.31646425\n","   73.71586012  78.80957523  83.31822195  95.32153624  68.41700906\n","  100.53226504 121.28850597 109.13928658  74.97411917  89.78865783\n","  125.00694835 112.56447075  86.50937875  94.2669651   78.57554787\n","  130.74496271 100.56115645  89.14435858 136.62888217  92.62732557\n","   89.08512772 105.04091175 113.76494698  90.57886217  78.92658891\n","  143.45035908  80.03894287  77.81423494  98.10242116 121.25961456\n","   90.22782113 121.6106556  113.44279735  89.29026367 127.75894185\n","  112.03718518 101.00031975  95.35042766 107.47075563  71.72517954\n","   65.10883857 100.7662924   73.71586012 113.6479333  126.55846562\n","   53.4276739   88.14901829 121.14260088 131.65218073 107.41152478\n","   91.80822981  98.98074777 113.88196066  79.21984712  89.58352188\n","  103.75231326 116.95610379  93.56488303  95.35042766  98.54158446\n","   60.71720553 103.48939448  91.48608019  81.00539175 109.78358583\n","   85.95320177 109.34442253 117.07311746 102.20079599  85.19188884\n","  104.71876213 140.81537926 130.33469082 121.96169663  98.13131257\n","   89.49539961  68.27110396  80.85948666  91.0758083  121.6106556\n","   89.29026367  95.23341398 107.03159233  66.86549179 126.90950665\n","   87.59284131 118.94678436  97.89728521 101.61572759  63.46919903\n","   92.3644068   96.55090389  97.89728521 113.67682471  84.28467082\n","  107.47075563 102.31780967 121.05447861  74.91633635 134.75521527\n","  139.26386199  63.87947092  64.581553   129.45636421 116.60506275\n","  104.36772109  80.12706514 119.97101606 113.6479333   95.87771323\n","   60.65942271  89.05623631  89.81754924  97.54624418  69.00207745\n","   94.12106001  86.09910686 118.85866209 111.56913046 109.95838234\n","  105.06980317  81.12240543 118.85866209  74.88744493  92.59843415\n","  130.8908678  113.12064773  54.39412277 134.84333754  72.25246511\n","  104.51362618 117.07311746 109.11039517  88.58818159 116.10666859\n","  118.94678436  99.21477513 101.52760532  98.62970673  93.50565218\n","  121.08337003  86.07021545  66.19230112  82.85016723 119.97101606\n","   62.56198101  80.88837807  85.45480761 130.33469082 122.92814551\n","   87.18256942  95.32153624  92.59843415  82.0888543  108.37797365\n","  111.3351031  116.42881822 103.48939448  94.67723699  99.53692475\n","   95.23341398  97.4292305  102.40593193  89.84644065 128.1388743\n","   96.90194493 107.23672827 108.81713696  76.17459541  98.8926255\n","   89.72942697 115.75562755 114.87730095  97.8683938  120.38128795\n","  117.98033549  73.51072417 102.84509524  87.82686866  91.01802547\n","  111.54023905  94.23807369  87.85576008  91.80822981 128.46102392\n","   81.56156873  79.19095571 131.35892252  91.80822981  77.22916655\n","  105.27493911 117.65818586 125.68013901 118.06845775  94.67723699]]\n"]}]},{"cell_type":"code","source":["#To-do-3:\n","#1. Split the dataset into training and test sets\n","from sklearn.model_selection import train_test_split\n","x = x.T\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","print(\"Shape of x_train: \", x_train.shape)\n","print(\"Shape of x_test: \", x_test.shape)\n","print(\"Shape of y_train: \", y_train.shape)\n","print(\"Shape of y_test: \", y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mFI68-fxhLmD","executionInfo":{"status":"ok","timestamp":1736048217288,"user_tz":-345,"elapsed":6,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"e64a171b-ed4b-4120-9c3f-82ef90c4f1e5"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of x_train:  (800, 2)\n","Shape of x_test:  (200, 2)\n","Shape of y_train:  (800,)\n","Shape of y_test:  (200,)\n"]}]},{"cell_type":"code","source":["#3.1.2-Building a cost function\n","import numpy as np\n","# define the cost function\n","def cost_function(x, y, w):\n","  \"\"\"\n","  Parameters:\n","  This function finds the Mean Square Error\n","  Input parameters:\n","    x: Feature Matrix\n","    y: Target Matrix\n","    w: Weight Matrix\n","  Output Parameters:\n","    cost: accumulated mean square error\n","  \"\"\"\n","  # number of training examples\n","  n = len(y)\n","\n","  # hypothesis: predicted values\n","  y_pred = np.dot(x, w)\n","\n","  # comute the squared errors\n","  squared_errors = np.square(y_pred - y)\n","\n","  # compute the mean squared error\n","  cost = (1 / (2 * n)) * np.sum(squared_errors)\n","\n","  return cost"],"metadata":{"id":"f2680ATHacgL","executionInfo":{"status":"ok","timestamp":1736048367720,"user_tz":-345,"elapsed":484,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["# Test case\n","x_test = np.array([[1,2], [3,4], [5,6]])\n","y_test = np.array([3,7,11])\n","w_test = np.array([1,1])\n","cost = cost_function(x_test, y_test, w_test)\n","if cost == 0:\n","  print(\"Proceed Further\")\n","else:\n","  print(\"Something went wrong: Reimplement a cost function\")\n","print(\"Cost function output: \", cost_function(x_test, y_test, w_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSYWEzW4i9vI","executionInfo":{"status":"ok","timestamp":1736048371541,"user_tz":-345,"elapsed":443,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"538f309f-b595-4297-f10b-bb33394e173e"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Proceed Further\n","Cost function output:  0.0\n"]}]},{"cell_type":"code","source":["#3.1.3 - Gradient Descent for Simple Linear Regression\n","def gradient_descent(x, y, w, alpha, iterations):\n","  \"\"\"\n","  Perform gradient descent to optimize the parameters of a linear regression model.\n","  Parameters:\n","    x (numpy.ndarray): Feature matrix (m x n).\n","    y (numpy.ndarray): Target vector (m x 1).\n","    w (numpy.ndarray): Initial guess for parameters (n x 1).\n","    alpha (float): Learning rate.\n","    iterations (int): Number of iterations for gradient decsent.\n","  Returns:\n","    tuple: A tuple containing the final optimized parameters (w_update) and the history of cost values\n","    (cost_history).w_update (numpy.ndarray): Updated parameters (n x 1).\n","    cost_history (list): History of cost values over iterations.\n","  \"\"\"\n","  # Initialize cost history\n","  cost_history = [0] * iterations\n","\n","  # number of sample\n","  m = len(y)\n","  # # initialize w_update as a copy of w (so we don't modify the original w)\n","  # w_update = w.copy()\n","  for iteration in range(iterations):\n","    # Step1: Hypothesis Values\n","    y_pred = np.dot(x, w) #predicted values\n","\n","    # Step2: Difference between Hypothesis and Actual y\n","    loss = y_pred - y\n","\n","    # Step3: Gradient Calculation\n","    dw = (1 / m) * np.dot(x.T, loss) #gradient\n","\n","    # Step4: Updating Values of w usng Gradient\n","    w_update = w - alpha * dw #update weights\n","\n","    # Step5: New Cost Value\n","    cost = cost_function(x, y, w_update)\n","    cost_history[iteration] = cost\n","\n","    # update W for the next iteration\n","    w = w_update\n","  return w_update, cost_history"],"metadata":{"id":"axdtJAkFRQgb","executionInfo":{"status":"ok","timestamp":1736055276870,"user_tz":-345,"elapsed":440,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}}},"execution_count":73,"outputs":[]},{"cell_type":"code","source":["# generate random test data\n","np.random.seed(0) #for reproductibility\n","x = np.random.rand(100, 3) #100 samples, 3 features\n","y = np.random.rand(100)\n","w = np.random.rand(3) #initial guess for parameters\n","# set hyperparameters\n","alpha = 0.01\n","iterations = 1000\n","# test the gradient_descent_function\n","final_params, cost_history = gradient_descent(x, y, w, alpha, iterations)\n","print(\"Final parameters: \", final_params)\n","print(\"Cost History: \", cost_history)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UeHs7kz-jHul","executionInfo":{"status":"ok","timestamp":1736048412629,"user_tz":-345,"elapsed":602,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"22e1f832-e289-4bb1-91cf-08d6634fba36"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Final parameters:  [-0.93473545 -0.49353388 -1.12641939]\n","Cost History:  [0.10724811113788572, 0.10661272433749933, 0.1059818141946656, 0.10535538070938447, 0.10473342388165595, 0.10411594371148007, 0.1035029401988568, 0.10289441334378614, 0.10229036314626813, 0.10169078960630272, 0.10109569272388992, 0.1005050724990298, 0.09991892893172223, 0.09933726202196734, 0.09876007176976502, 0.09818735817511534, 0.09761912123801829, 0.09705536095847384, 0.09649607733648204, 0.09594127037204285, 0.09539094006515626, 0.09484508641582233, 0.09430370942404101, 0.0937668090898123, 0.09323438541313618, 0.09270643839401274, 0.0921829680324419, 0.09166397432842366, 0.09114945728195809, 0.09063941689304508, 0.09013385316168472, 0.08963276608787699, 0.08913615567162188, 0.0886440219129194, 0.08815636481176953, 0.08767318436817226, 0.08719448058212761, 0.0867202534536356, 0.08625050298269621, 0.08578522916930945, 0.0853244320134753, 0.08486811151519376, 0.08441626767446486, 0.08396890049128858, 0.08352600996566491, 0.08308759609759386, 0.08265365888707545, 0.08222419833410965, 0.08179921443869649, 0.08137870720083591, 0.08096267662052796, 0.08055112269777266, 0.08014404543256996, 0.07974144482491988, 0.07934332087482243, 0.07894967358227761, 0.07856050294728541, 0.0781758089698458, 0.07779559164995885, 0.0774198509876245, 0.07704858698284275, 0.07668179963561367, 0.0763194889459372, 0.07596165491381332, 0.07560829753924207, 0.07525941682222347, 0.07491501276275746, 0.07457508536084409, 0.07423963461648332, 0.07390866052967518, 0.07358216310041968, 0.07326014232871679, 0.0729425982145665, 0.07262953075796885, 0.07232093995892384, 0.07201682581743143, 0.07171718833349164, 0.07142202750710448, 0.07113134333826994, 0.07084513582698801, 0.0705634049732587, 0.07028615077708202, 0.07001337323845798, 0.06974507235738653, 0.06948124813386772, 0.06922190056790152, 0.06896702965948795, 0.06871663540862699, 0.06847071781531866, 0.06822927687956294, 0.06799231260135985, 0.06775982498070938, 0.06753181401761153, 0.06730827971206631, 0.06708922206407371, 0.06687464107363372, 0.06666453674074636, 0.06645890906541162, 0.0662577580476295, 0.06606108368739999, 0.06586888598472312, 0.06568116493959886, 0.06549792055202722, 0.0653191528220082, 0.06514486174954182, 0.06497504733462803, 0.06480970957726688, 0.06464884847745836, 0.06449246403520245, 0.06434055625049916, 0.0641931251233485, 0.06405017065375045, 0.06391169284170503, 0.06377769168721223, 0.06364816719027203, 0.06352311935088448, 0.06340254816904956, 0.06328645364476723, 0.06317483577803752, 0.06306769456886044, 0.062965030017236, 0.06286684212316417, 0.06277313088664492, 0.06268389630767837, 0.06259913838626438, 0.06251885712240303, 0.06244305251609429, 0.062371724567338215, 0.0623048732761347, 0.062242498642483844, 0.062184600666385606, 0.06213117934783998, 0.062082234686846996, 0.06203776668340659, 0.06199777533751884, 0.06196226064918369, 0.061931222618401185, 0.06190466124517129, 0.061882576529494, 0.061864968471369366, 0.06185183707079733, 0.06184318232777791, 0.06183900424231113, 0.06183930281439696, 0.061844078044035404, 0.06185332993122648, 0.06186705847597019, 0.061885263678266494, 0.061907945538115426, 0.06193510405551699, 0.06196673923047118, 0.062002851062977975, 0.06204343955303738, 0.062088504700649444, 0.0621380465058141, 0.0621920649685314, 0.0622505600888013, 0.06231353186662383, 0.06238098030199898, 0.06245290539492675, 0.06252930714540714, 0.06261018555344014, 0.06269554061902578, 0.06278537234216404, 0.06287968072285491, 0.0629784657610984, 0.06308172745689453, 0.06318946581024326, 0.06330168082114462, 0.06341837248959861, 0.0635395408156052, 0.06366518579916443, 0.06379530744027627, 0.06392990573894074, 0.06406898069515783, 0.06421253230892754, 0.06436056058024986, 0.06451306550912482, 0.06467004709555238, 0.06483150533953257, 0.0649974402410654, 0.06516785180015083, 0.06534274001678887, 0.06552210489097957, 0.06570594642272286, 0.06589426461201878, 0.06608705945886734, 0.0662843309632685, 0.06648607912522227, 0.06669230394472868, 0.0669030054217877, 0.06711818355639933, 0.0673378383485636, 0.06756196979828051, 0.06779057790555001, 0.06802366267037215, 0.0682612240927469, 0.06850326217267429, 0.06874977691015427, 0.06900076830518687, 0.06925623635777212, 0.06951618106790998, 0.06978060243560046, 0.07004950046084356, 0.07032287514363927, 0.07060072648398763, 0.07088305448188859, 0.07116985913734217, 0.07146114045034838, 0.07175689842090721, 0.07205713304901865, 0.07236184433468273, 0.07267103227789942, 0.07298469687866874, 0.07330283813699066, 0.07362545605286522, 0.07395255062629241, 0.07428412185727219, 0.07462016974580463, 0.07496069429188966, 0.07530569549552732, 0.07565517335671759, 0.0760091278754605, 0.07636755905175603, 0.07673046688560418, 0.07709785137700495, 0.07746971252595834, 0.07784605033246435, 0.07822686479652297, 0.07861215591813424, 0.07900192369729811, 0.0793961681340146, 0.0797948892282837, 0.08019808698010544, 0.0806057613894798, 0.0810179124564068, 0.08143454018088639, 0.0818556445629186, 0.08228122560250345, 0.0827112832996409, 0.083145817654331, 0.08358482866657373, 0.08402831633636904, 0.08447628066371697, 0.08492872164861756, 0.08538563929107076, 0.08584703359107655, 0.08631290454863498, 0.08678325216374605, 0.08725807643640972, 0.08773737736662601, 0.08822115495439495, 0.0887094091997165, 0.08920214010259066, 0.08969934766301745, 0.09020103188099682, 0.09070719275652885, 0.09121783028961349, 0.09173294448025074, 0.09225253532844063, 0.09277660283418315, 0.09330514699747829, 0.09383816781832603, 0.09437566529672642, 0.0949176394326794, 0.095464090226185, 0.09601501767724324, 0.09657042178585408, 0.09713030255201754, 0.09769465997573366, 0.09826349405700237, 0.09883680479582373, 0.09941459219219768, 0.0999968562461243, 0.10058359695760348, 0.1011748143266353, 0.10177050835321977, 0.10237067903735682, 0.10297532637904651, 0.10358445037828883, 0.10419805103508376, 0.1048161283494313, 0.10543868232133148, 0.10606571295078426, 0.10669722023778971, 0.10733320418234772, 0.10797366478445838, 0.10861860204412167, 0.10926801596133758, 0.10992190653610609, 0.11058027376842723, 0.111243117658301, 0.11191043820572735, 0.11258223541070639, 0.11325850927323801, 0.11393925979332227, 0.11462448697095914, 0.11531419080614864, 0.11600837129889073, 0.11670702844918548, 0.11741016225703284, 0.1181177727224328, 0.11882985984538541, 0.11954642362589062, 0.12026746406394846, 0.12099298115955893, 0.121722974912722, 0.12245744532343772, 0.123196392391706, 0.12393981611752697, 0.12468771650090055, 0.12544009354182675, 0.12619694724030553, 0.12695827759633696, 0.127724084609921, 0.12849436828105767, 0.129269128609747, 0.13004836559598892, 0.13083207923978343, 0.1316202695411306, 0.1324129365000304, 0.13321008011648278, 0.1340117003904878, 0.13481779732204544, 0.13562837091115568, 0.13644342115781857, 0.1372629480620341, 0.13808695162380225, 0.13891543184312297, 0.1397483887199963, 0.14058582225442232, 0.14142773244640092, 0.14227411929593214, 0.143124982803016, 0.1439803229676525, 0.14484013978984162, 0.1457044332695833, 0.14657320340687763, 0.1474464502017246, 0.14832417365412418, 0.1492063737640764, 0.1500930505315812, 0.15098420395663867, 0.1518798340392487, 0.15277994077941143, 0.15368452417712672, 0.15459358423239464, 0.15550712094521518, 0.15642513431558835, 0.15734762434351415, 0.15827459102899258, 0.15920603437202363, 0.16014195437260728, 0.16108235103074356, 0.1620272243464325, 0.1629765743196739, 0.16393040095046807, 0.16488870423881488, 0.16585148418471424, 0.16681874078816625, 0.16779047404917088, 0.16876668396772812, 0.16974737054383796, 0.17073253377750053, 0.17172217366871562, 0.17271629021748333, 0.17371488342380367, 0.17471795328767667, 0.1757254998091023, 0.1767375229880805, 0.17775402282461125, 0.17877499931869476, 0.17980045247033086, 0.1808303822795195, 0.18186478874626083, 0.18290367187055484, 0.1839470316524014, 0.18499486809180055, 0.18604718118875233, 0.18710397094325681, 0.18816523735531387, 0.18923098042492353, 0.1903012001520858, 0.19137589653680073, 0.1924550695790683, 0.19353871927888847, 0.1946268456362612, 0.1957194486511866, 0.1968165283236646, 0.1979180846536953, 0.19902411764127856, 0.20013462728641437, 0.20124961358910295, 0.20236907654934402, 0.20349301616713775, 0.20462143244248418, 0.20575432537538316, 0.20689169496583476, 0.20803354121383894, 0.20917986411939582, 0.21033066368250528, 0.21148593990316739, 0.21264569278138212, 0.21380992231714943, 0.2149786285104694, 0.21615181136134193, 0.21732947086976714, 0.21851160703574501, 0.21969821985927543, 0.22088930934035844, 0.22208487547899416, 0.22328491827518246, 0.22448943772892335, 0.22569843384021693, 0.22691190660906313, 0.22812985603546185, 0.22935228211941325, 0.23057918486091733, 0.23181056425997393, 0.2330464203165832, 0.23428675303074514, 0.23553156240245965, 0.23678084843172678, 0.2380346111185466, 0.23929285046291893, 0.24055556646484388, 0.2418227591243215, 0.24309442844135176, 0.24437057441593468, 0.24565119704807012, 0.2469362963377582, 0.24822587228499893, 0.2495199248897923, 0.2508184541521382, 0.25212146007203684, 0.25342894264948806, 0.25474090188449194, 0.25605733777704837, 0.2573782503271574, 0.2587036395348191, 0.26003350540003345, 0.2613678479228004, 0.2627066671031199, 0.26404996294099214, 0.26539773543641687, 0.2667499845893943, 0.26810671039992434, 0.26946791286800703, 0.2708335919936423, 0.2722037477768302, 0.27357838021757075, 0.2749574893158639, 0.27634107507170963, 0.2777291374851081, 0.279121676556059, 0.2805186922845627, 0.28192018467061897, 0.28332615371422787, 0.2847365994153893, 0.28615152177410347, 0.2875709207903702, 0.2889947964641895, 0.2904231487955616, 0.2918559777844862, 0.2932932834309634, 0.2947350657349932, 0.29618132469657565, 0.2976320603157108, 0.2990872725923985, 0.30054696152663885, 0.30201112711843175, 0.30347976936777726, 0.3049528882746755, 0.3064304838391264, 0.3079125560611297, 0.30939910494068584, 0.3108901304777944, 0.3123856326724558, 0.31388561152466976, 0.3153900670344363, 0.3168989992017554, 0.3184124080266271, 0.31993029350905156, 0.32145265564902864, 0.3229794944465582, 0.3245108099016406, 0.3260466020142754, 0.3275868707844629, 0.32913161621220305, 0.3306808382974959, 0.3322345370403412, 0.3337927124407392, 0.33535536449868986, 0.3369224932141932, 0.33849409858724905, 0.3400701806178576, 0.34165073930601864, 0.34323577465173233, 0.3448252866549988, 0.3464192753158178, 0.3480177406341894, 0.3496206826101136, 0.3512281012435905, 0.3528399965346199, 0.354456368483202, 0.35607721708933676, 0.3577025423530241, 0.3593323442742641, 0.3609666228530567, 0.3626053780894018, 0.36424860998329955, 0.3658963185347501, 0.36754850374375314, 0.36920516561030886, 0.3708663041344171, 0.3725319193160781, 0.3742020111552916, 0.37587657965205773, 0.37755562480637667, 0.37923914661824804, 0.380927145087672, 0.38261962021464874, 0.38431657199917796, 0.3860180004412599, 0.3877239055408944, 0.3894342872980816, 0.3911491457128212, 0.3928684807851137, 0.3945922925149587, 0.3963205809023564, 0.39805334594730646, 0.3997905876498095, 0.401532306009865, 0.4032785010274731, 0.4050291727026338, 0.4067843210353472, 0.40854394602561306, 0.4103080476734317, 0.4120766259788029, 0.4138496809417267, 0.4156272125622032, 0.41740922084023224, 0.419195705775814, 0.42098666736894835, 0.42278210561963525, 0.4245820205278748, 0.42638641209366696, 0.4281952803170118, 0.4300086251979094, 0.43182644673635945, 0.433648744932362, 0.43547551978591725, 0.43730677129702517, 0.43914249946568584, 0.44098270429189895, 0.4428273857756647, 0.4446765439169832, 0.44653017871585415, 0.44838829017227777, 0.450250878286254, 0.4521179430577829, 0.4539894844868644, 0.4558655025734985, 0.4577459973176854, 0.4596309687194246, 0.46152041677871675, 0.4634143414955613, 0.46531274286995855, 0.4672156209019083, 0.4691229755914108, 0.47103480693846594, 0.47295111494307374, 0.47487189960523396, 0.47679716092494695, 0.4787268989022126, 0.4806611135370308, 0.48259980482940157, 0.4845429727793251, 0.4864906173868011, 0.4884427386518298, 0.4903993365744112, 0.49236041115454515, 0.49432596239223164, 0.4962959902874709, 0.4982704948402627, 0.5002494760506069, 0.502232933918504, 0.5042208684439536, 0.5062132796269561, 0.5082101674675108, 0.5102115319656184, 0.5122173731212785, 0.5142276909344914, 0.5162424854052567, 0.5182617565335746, 0.5202855043194454, 0.5223137287628685, 0.5243464298638446, 0.526383607622373, 0.5284252620384542, 0.5304713931120879, 0.5325220008432743, 0.5345770852320131, 0.5366366462783048, 0.5387006839821491, 0.540769198343546, 0.5428421893624954, 0.5449196570389975, 0.5470016013730521, 0.5490880223646595, 0.5511789200138194, 0.553274294320532, 0.5553741452847972, 0.557478472906615, 0.5595872771859854, 0.5617005581229084, 0.5638183157173842, 0.5659405499694125, 0.5680672608789934, 0.5701984484461269, 0.5723341126708131, 0.574474253553052, 0.5766188710928433, 0.5787679652901874, 0.5809215361450839, 0.5830795836575333, 0.5852421078275353, 0.5874091086550897, 0.5895805861401968, 0.5917565402828566, 0.5939369710830689, 0.596121878540834, 0.5983112626561516, 0.600505123429022, 0.6027034608594447, 0.6049062749474201, 0.6071135656929483, 0.609325333096029, 0.6115415771566624, 0.6137622978748483, 0.6159874952505869, 0.6182171692838783, 0.6204513199747221, 0.6226899473231184, 0.6249330513290675, 0.6271806319925692, 0.6294326893136234, 0.6316892232922304, 0.63395023392839, 0.6362157212221022, 0.638485685173367, 0.6407601257821844, 0.6430390430485545, 0.6453224369724773, 0.6476103075539524, 0.6499026547929805, 0.652199478689561, 0.654500779243694, 0.65680655645538, 0.6591168103246181, 0.6614315408514092, 0.6637507480357531, 0.6660744318776491, 0.668402592377098, 0.6707352295340995, 0.6730723433486537, 0.6754139338207605, 0.6777600009504198, 0.6801105447376318, 0.6824655651823963, 0.6848250622847136, 0.6871890360445835, 0.6895574864620061, 0.691930413536981, 0.6943078172695089, 0.6966896976595892, 0.6990760547072222, 0.7014668884124078, 0.7038621987751461, 0.706261985795437, 0.7086662494732804, 0.7110749898086764, 0.7134882068016254, 0.7159059004521263, 0.7183280707601805, 0.7207547177257871, 0.7231858413489464, 0.7256214416296581, 0.7280615185679226, 0.7305060721637398, 0.7329551024171095, 0.7354086093280315, 0.7378665928965065, 0.740329053122534, 0.7427959900061144, 0.7452674035472471, 0.7477432937459325, 0.7502236606021707, 0.7527085041159614, 0.7551978242873045, 0.7576916211162005, 0.760189894602649, 0.7626926447466501, 0.7651998715482037, 0.7677115750073102, 0.770227755123969, 0.7727484118981809, 0.7752735453299451, 0.777803155419262, 0.7803372421661315, 0.7828758055705537, 0.7854188456325284, 0.7879663623520557, 0.7905183557291356, 0.7930748257637684, 0.7956357724559535, 0.7982011958056916, 0.8007710958129819, 0.803345472477825, 0.8059243258002207, 0.8085076557801693, 0.8110954624176699, 0.8136877457127235, 0.8162845056653296, 0.8188857422754886, 0.8214914555432, 0.8241016454684642, 0.8267163120512807, 0.8293354552916502, 0.831959075189572, 0.8345871717450467, 0.8372197449580736, 0.8398567948286533, 0.842498321356786, 0.8451443245424709, 0.8477948043857084, 0.8504497608864989, 0.8531091940448419, 0.8557731038607373, 0.8584414903341854, 0.8611143534651862, 0.8637916932537397, 0.8664735096998455, 0.8691598028035041, 0.8718505725647152, 0.8745458189834792, 0.8772455420597959, 0.8799497417936647, 0.8826584181850865, 0.8853715712340611, 0.8880892009405877, 0.8908113073046675, 0.8935378903262995, 0.8962689500054846, 0.8990044863422222, 0.9017444993365122, 0.904488988988355, 0.9072379552977501, 0.9099913982646982, 0.9127493178891986, 0.9155117141712518, 0.9182785871108579, 0.9210499367080162, 0.9238257629627272, 0.9266060658749907, 0.9293908454448072, 0.9321801016721761, 0.9349738345570975, 0.9377720440995718, 0.9405747302995987, 0.9433818931571779, 0.9461935326723099, 0.9490096488449946, 0.9518302416752319, 0.9546553111630216, 0.9574848573083645, 0.9603188801112594, 0.9631573795717073, 0.9660003556897075, 0.9688478084652604, 0.971699737898366, 0.9745561439890245, 0.9774170267372353, 0.9802823861429988, 0.9831522222063148, 0.9860265349271835, 0.9889053243056048, 0.9917885903415785, 0.9946763330351055, 0.9975685523861845, 1.0004652483948164, 1.003366421061001, 1.0062720703847379, 1.0091821963660277, 1.0120967990048702, 1.0150158783012648, 1.0179394342552122, 1.0208674668667126, 1.0237999761357655, 1.026736962062371, 1.029678424646529, 1.0326243638882397, 1.0355747797875028, 1.0385296723443187, 1.0414890415586873, 1.0444528874306085, 1.0474212099600824, 1.0503940091471085, 1.0533712849916876, 1.0563530374938193, 1.0593392666535035, 1.0623299724707402, 1.06532515494553, 1.068324814077872, 1.0713289498677667, 1.0743375623152143, 1.0773506514202142, 1.0803682171827667, 1.083390259602872, 1.0864167786805299, 1.0894477744157403, 1.0924832468085033, 1.095523195858819, 1.0985676215666873, 1.1016165239321083, 1.104669902955082, 1.1077277586356082, 1.110790090973687, 1.1138568999693186, 1.1169281856225024, 1.1200039479332393, 1.1230841869015287, 1.1261689025273705, 1.1292580948107651, 1.1323517637517122, 1.135449909350212, 1.1385525316062646, 1.1416596305198694, 1.1447712060910271, 1.1478872583197375, 1.1510077872060003, 1.1541327927498157, 1.1572622749511838, 1.1603962338101048, 1.1635346693265782, 1.166677581500604, 1.1698249703321828, 1.172976835821314, 1.176133177967998, 1.1792939967722342, 1.1824592922340236, 1.1856290643533651, 1.1888033131302596, 1.1919820385647066, 1.1951652406567064, 1.1983529194062583, 1.2015450748133634, 1.2047417068780206, 1.2079428156002308, 1.2111484009799935, 1.2143584630173088, 1.2175730017121769, 1.220792017064597, 1.2240155090745706, 1.2272434777420962, 1.2304759230671747, 1.2337128450498058, 1.2369542436899894, 1.2402001189877256, 1.243450470943015, 1.2467052995558563, 1.2499646048262505, 1.2532283867541973, 1.2564966453396964, 1.2597693805827486, 1.2630465924833534, 1.2663282810415106, 1.2696144462572203, 1.272905088130483, 1.276200206661298, 1.2794998018496657, 1.2828038736955862, 1.2861124221990594, 1.2894254473600848, 1.292742949178663, 1.296064927654794, 1.2993913827884773, 1.3027223145797135, 1.3060577230285026, 1.309397608134844, 1.3127419698987377, 1.316090808320184, 1.3194441233991836, 1.3228019151357353, 1.3261641835298397, 1.3295309285814971, 1.332902150290706, 1.3362778486574687, 1.3396580236817832, 1.3430426753636509, 1.346431803703071, 1.3498254087000436, 1.3532234903545688, 1.3566260486666468, 1.3600330836362775, 1.363444595263461, 1.366860583548197, 1.3702810484904848, 1.373705990090326, 1.3771354083477194, 1.3805693032626658, 1.3840076748351653, 1.3874505230652165, 1.3908978479528205, 1.3943496494979775, 1.397805927700687, 1.4012666825609492, 1.4047319140787635, 1.4082016222541305, 1.4116758070870505, 1.415154468577523, 1.4186376067255477, 1.4221252215311262, 1.4256173129942562, 1.4291138811149395, 1.4326149258931746, 1.4361204473289633, 1.439630445422304, 1.4431449201731976, 1.4466638715816436, 1.4501872996476421, 1.4537152043711936, 1.4572475857522975, 1.4607844437909543, 1.464325778487163, 1.4678715898409254, 1.4714218778522397, 1.4749766425211073, 1.478535883847527, 1.4820996018314987, 1.4856677964730238, 1.4892404677721012, 1.4928176157287314, 1.4963992403429145, 1.49998534161465, 1.5035759195439375, 1.507170974130778, 1.5107705053751717, 1.5143745132771176, 1.5179829978366157, 1.5215959590536667, 1.525213396928271, 1.5288353114604272, 1.5324617026501364, 1.536092570497398, 1.5397279150022123, 1.543367736164579, 1.5470120339844982, 1.5506608084619709, 1.5543140595969953, 1.5579717873895729, 1.5616339918397029, 1.565300672947385, 1.5689718307126208, 1.5726474651354085, 1.5763275762157485, 1.5800121639536422, 1.5837012283490877, 1.587394769402086, 1.5910927871126368, 1.5947952814807405, 1.5985022525063965, 1.6022137001896055, 1.6059296245303665, 1.609650025528681, 1.6133749031845475, 1.6171042574979668, 1.6208380884689384, 1.6245763960974637, 1.6283191803835408, 1.6320664413271704, 1.635818178928353, 1.6395743931870879, 1.6433350841033751, 1.6471002516772155, 1.6508698959086083, 1.6546440167975542, 1.6584226143440517, 1.6622056885481027, 1.6659932394097063, 1.6697852669288622, 1.6735817711055712, 1.677382751939832, 1.6811882094316462, 1.6849981435810124, 1.6888125543879315, 1.6926314418524033, 1.6964548059744275, 1.7002826467540044, 1.7041149641911342]\n"]}]},{"cell_type":"code","source":["#3.1.4 - Evaluate the Model\n","#1.Root Mean Square Error\n","\n","# Model Evaluation - RMSE\n","def rmse(y, y_pred):\n","  \"\"\"\n","  This function calculates the Root Mean Square.\n","  Input Arguments:\n","    y: array of actual (target) dependent variables\n","    y_pred: array of predicted dependent variables\n","  Output Arguments:\n","    rmse: Root Mean Square Error\n","  \"\"\"\n","  # calculate the squared differences\n","  squared_diff = (y - y_pred) ** 2\n","\n","  # calculate the mean squared error\n","  mse = np.mean(squared_diff)\n","\n","  # return the square root of the mean squared error\n","  rmse = np.sqrt(mse)\n","  return rmse"],"metadata":{"id":"aUKgw7BNRpwO","executionInfo":{"status":"ok","timestamp":1736048458174,"user_tz":-345,"elapsed":433,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["# Model Evaluation - R2\n","def r2(y, y_pred):\n","  \"\"\"\n","  This function calculates the R2 Squared Error.\n","  Input Arguments:\n","    y: array of actual (target) dependent variables\n","    y_pred: array of predicted dependent variables\n","  Output Arguments:\n","    rsquared: R squared error\n","  \"\"\"\n","  # mean of actual values\n","  mean_y = np.mean(y)\n","  # calculate SST (Total Sum of Squares)\n","  ss_tot = np.sum((y - mean_y) ** 2)\n","  # calculate SSR (Sum of Squared Residuals)\n","  ss_res = np.sum((y - y_pred) ** 2)\n","  # calculate R-squared\n","  r2 = 1 - (ss_res / ss_tot)\n","  return r2"],"metadata":{"id":"BjB8qmKYjT0u","executionInfo":{"status":"ok","timestamp":1736048490841,"user_tz":-345,"elapsed":423,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}}},"execution_count":47,"outputs":[]},{"cell_type":"code","source":["# test data\n","y = np.random.rand(100) #actual values\n","y_pred = np.random.rand(100) #predicted values from the model\n","\n","# calculate RMSE\n","rmse_value = rmse(y, y_pred)\n","print(\"RMSE: \", rmse_value)\n","\n","# calculate R2\n","r2_value = r2(y, y_pred)\n","print(\"R2: \", r2_value)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7McJ7QV3jbwW","executionInfo":{"status":"ok","timestamp":1736048494361,"user_tz":-345,"elapsed":476,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"54eb06d0-633f-45e3-a145-81416708efa0"},"execution_count":48,"outputs":[{"output_type":"stream","name":"stdout","text":["RMSE:  0.431377337944294\n","R2:  -1.1215707120566534\n"]}]},{"cell_type":"code","source":["#3.1.5: Step5: Main function to integrate all steps\n","def main():\n","  # Step1: Load the dataset\n","  df = pd.read_csv(\"/content/drive/MyDrive/AIWeek2/student.csv\")\n","\n","  # Step2: Split the data into features(x) and target(y)\n","  x = df[['Math', 'Reading']].values #features: Math and Reading\n","  y = df['Writing'].values #target: Writing\n","\n","  # Step3: Split the data into training and test sets (80% train, 20% test)\n","  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","\n","  # Step4: Initialize weights(W) to zeros, learning rate and number of iterations\n","  W = np.zeros(x_train.shape[1]) #initialize weights\n","  alpha = 0.00001 #learning rate\n","  iterations = 1000 #number of iterations for gradient descent\n","\n","  # Step5: Perform gradient descent\n","  W_optimal, cost_history = gradient_descent(x_train, y_train, W, alpha, iterations)\n","\n","  # Step6: Make prediction on the test set\n","  y_pred = np.dot(x_test, W_optimal)\n","\n","  # Step7: Evaluate the model using RMSE and R-Squared\n","  model_rmse = rmse(y_test, y_pred)\n","  model_r2 = r2(y_test, y_pred)\n","\n","  # Step8: Output the results\n","  print(\"Final Weights: \", W_optimal)\n","  print(\"Cost history (first 10 iterations): \", cost_history[:10])\n","  print(\"R-Squared on test set: \", model_r2)\n","\n","# Execute the main function\n","if __name__ == \"__main__\":\n","  main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRMSBf2CjdSA","executionInfo":{"status":"ok","timestamp":1736055092991,"user_tz":-345,"elapsed":439,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"8f31e76b-9bf5-4702-ffa8-e3afb954c842"},"execution_count":71,"outputs":[{"output_type":"stream","name":"stdout","text":["Final Weights:  [0.34811659 0.64614558]\n","Cost history (first 10 iterations):  [2013.165570783755, 1640.286832599692, 1337.0619994901588, 1090.4794892850578, 889.9583270083234, 726.8940993009545, 594.2897260808594, 486.4552052951635, 398.7634463599484, 327.4517147324688]\n","R-Squared on test set:  0.8886354462786421\n"]}]},{"cell_type":"markdown","source":["#1. Did your model overfitt, underfitt, or performance is acceptable\n","-Final Weights: These values are not extreme and suggest that the model is capturing the relationships between the features (math and Reading) and the target(Writing)\n","\n","-Cost History: The cost history shows a steady decrease over the iterations, which is a good sign.\n","\n","-R-Squared on test set: It suggests the model is able to explain about 88.85% of the variance in the test data. Generally, an R-Squared values closer to 1 signifies a better fit, so this is a good result.\n","\n","Conclusion:\n","1. Overfitting: Overfitting occurs when the model performs poorly on the test set. Since R-Squared on test set = 0.89, which is on the high end, the model do nit show signs of overfitting.\n","\n","2. Underfitting: This would occur if the model were to simple t capture the underlying relationships in the data. Guven the R-Squared value is relatively high, there is no indication of underfitting.\n","\n","3. Acceptable performance: Based on the steady decrease in the cost function, the final weights, adn the R-Squared value of 0.89, the performance is acceptable. The model is fitting the data well without overfitting or underfitting.\n","\n","\n","Hence, the model's performance is acceptable."],"metadata":{"id":"xIM5joelmCmo"}},{"cell_type":"markdown","source":["#2. Experimentation with different value of learning rate, making it higher and lower"],"metadata":{"id":"AwphdZhvnAmo"}},{"cell_type":"code","source":["#3.1.5: Step5: Main function to integrate all steps\n","def main():\n","  # Step1: Load the dataset\n","  df = pd.read_csv(\"/content/drive/MyDrive/AIWeek2/student.csv\")\n","\n","  # Step2: Split the data into features(x) and target(y)\n","  x = df[['Math', 'Reading']].values #features: Math and Reading\n","  y = df['Writing'].values #target: Writing\n","\n","  # Step3: Split the data into training and test sets (80% train, 20% test)\n","  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","\n","  # Step4: Initialize weights(W) to zeros, learning rate and number of iterations\n","  W = np.zeros(x_train.shape[1]) #initialize weights\n","  alpha = 0.0001 # increasing learning rate\n","  iterations = 1000 #number of iterations for gradient descent\n","\n","  # Step5: Perform gradient descent\n","  W_optimal, cost_history = gradient_descent(x_train, y_train, W, alpha, iterations)\n","\n","  # Step6: Make prediction on the test set\n","  y_pred = np.dot(x_test, W_optimal)\n","\n","  # Step7: Evaluate the model using RMSE and R-Squared\n","  model_rmse = rmse(y_test, y_pred)\n","  model_r2 = r2(y_test, y_pred)\n","\n","  # Step8: Output the results\n","  print(\"Final Weights: \", W_optimal)\n","  print(\"Cost history (first 10 iterations): \", cost_history[:10])\n","  print(\"R-Squared on test set: \", model_r2)\n","\n","# Execute the main function\n","if __name__ == \"__main__\":\n","  main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oa-UIkOonPHy","executionInfo":{"status":"ok","timestamp":1736055843207,"user_tz":-345,"elapsed":463,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"9f6e0b8d-18b7-48f9-f73d-39de2fef8dc0"},"execution_count":78,"outputs":[{"output_type":"stream","name":"stdout","text":["Final Weights:  [0.0894932  0.89504864]\n","Cost history (first 10 iterations):  [17.813797177522098, 16.983149024878305, 16.925140245010397, 16.867870818076216, 16.811093513105355, 16.754804026075387, 16.69899816573971, 16.64367177688582, 16.588820740001896, 16.53444097097003]\n","R-Squared on test set:  0.908240340333986\n"]}]},{"cell_type":"markdown","source":["When we increase the value of alpha to 0.0001, the model is seems to perform well indicated by:\n","-a high R-squared score, suggesting a good fit\n","-a steady decrease in cost, showing that the modle is learnig and converging over iterations\n","-the weights are reasonable, showing how the features Math and Reading influence the target Writing.\n"],"metadata":{"id":"UO0kEX4HnOl7"}},{"cell_type":"code","source":["#3.1.5: Step5: Main function to integrate all steps\n","def main():\n","  # Step1: Load the dataset\n","  df = pd.read_csv(\"/content/drive/MyDrive/AIWeek2/student.csv\")\n","\n","  # Step2: Split the data into features(x) and target(y)\n","  x = df[['Math', 'Reading']].values #features: Math and Reading\n","  y = df['Writing'].values #target: Writing\n","\n","  # Step3: Split the data into training and test sets (80% train, 20% test)\n","  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n","\n","  # Step4: Initialize weights(W) to zeros, learning rate and number of iterations\n","  W = np.zeros(x_train.shape[1]) #initialize weights\n","  alpha = 0.0000001 # decreasing learning rate\n","  iterations = 1000 #number of iterations for gradient descent\n","\n","  # Step5: Perform gradient descent\n","  W_optimal, cost_history = gradient_descent(x_train, y_train, W, alpha, iterations)\n","\n","  # Step6: Make prediction on the test set\n","  y_pred = np.dot(x_test, W_optimal)\n","\n","  # Step7: Evaluate the model using RMSE and R-Squared\n","  model_rmse = rmse(y_test, y_pred)\n","  model_r2 = r2(y_test, y_pred)\n","\n","  # Step8: Output the results\n","  print(\"Final Weights: \", W_optimal)\n","  print(\"Cost history (first 10 iterations): \", cost_history[:10])\n","  print(\"R-Squared on test set: \", model_r2)\n","\n","# Execute the main function\n","if __name__ == \"__main__\":\n","  main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XpzxTgO6oVkV","executionInfo":{"status":"ok","timestamp":1736055792791,"user_tz":-345,"elapsed":549,"user":{"displayName":"Nikisha Shrestha","userId":"08931636431601053448"}},"outputId":"99f77f7c-7599-411e-89db-3f3836a0b290"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["Final Weights:  [0.30499961 0.32040295]\n","Cost history (first 10 iterations):  [2466.87895905551, 2462.068631905341, 2457.267749966842, 2452.476294693852, 2447.6942475766255, 2442.921590141761, 2438.158303952132, 2433.404370606813, 2428.6597717410104, 2423.92448902599]\n","R-Squared on test set:  -1.8954840865170746\n"]}]},{"cell_type":"markdown","source":["When we decrease the value of alpha to 0.0000001:\n","-the learning rate is so small that each weight update is miniscule.\n","-the model makes only negligible progress in reducing the cost function during the 1000 iterations.\n","-consequently, the resulting weights are not optimal, and the predictions on the test set are inaccurate, leading to poor evaluation metrics such as the negavtive R-squared score."],"metadata":{"id":"68-_5K1hojw1"}}]}