{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Building a Cost Function:"
      ],
      "metadata": {
        "id": "s0QL-FTQMgS0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjNvb03WMYaO",
        "outputId": "d728a4f6-23a0-42e5-c539-9d67f407bcdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 25.5\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the cost function (Mean Squared Error)\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    This function calculates the Mean Squared Error (MSE).\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Weight vector (n x 1).\n",
        "\n",
        "    Returns:\n",
        "    float: The mean squared error (MSE).\n",
        "    \"\"\"\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    # Calculate the predicted values (hypothesis)\n",
        "    Y_pred = np.dot(X, W)  # X * W\n",
        "\n",
        "    # Calculate the squared errors\n",
        "    squared_errors = (Y_pred - Y) ** 2\n",
        "\n",
        "    # Calculate the mean squared error\n",
        "    cost = (1 / (2 * m)) * np.sum(squared_errors)\n",
        "\n",
        "    return cost\n",
        "\n",
        "# Example feature matrix (X), target vector (Y), and weight vector (W)\n",
        "X = np.array([[6, 2], [2, 5], [2, 6]])  # 3 samples, 2 features\n",
        "Y = np.array([[7], [13], [17]])  # 3 target values\n",
        "W = np.array([[0.5], [1]])  # Initial weights for the 2 features\n",
        "\n",
        "# Call the cost function to calculate MSE\n",
        "cost_value = cost_function(X, Y, W)\n",
        "\n",
        "# Output the MSE result\n",
        "print(\"Mean Squared Error:\", cost_value)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Descent from Scratch:"
      ],
      "metadata": {
        "id": "I6sT39vjPaVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the cost function (Mean Squared Error)\n",
        "def cost_function(X, Y, W):\n",
        "    \"\"\"\n",
        "    This function calculates the Mean Squared Error (MSE).\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Weight vector (n x 1).\n",
        "\n",
        "    Returns:\n",
        "    float: The mean squared error (MSE).\n",
        "    \"\"\"\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    # Calculate the predicted values (hypothesis)\n",
        "    Y_pred = np.dot(X, W)  # X * W\n",
        "\n",
        "    # Calculate the squared errors\n",
        "    squared_errors = (Y_pred - Y) ** 2\n",
        "\n",
        "    # Calculate the mean squared error\n",
        "    cost = (1 / (2 * m)) * np.sum(squared_errors)\n",
        "\n",
        "    return cost\n",
        "\n",
        "# Define the gradient descent function\n",
        "def gradient_descent(X, Y, W, alpha, iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to optimize the parameters of a linear regression model.\n",
        "\n",
        "    Parameters:\n",
        "    X (numpy.ndarray): Feature matrix (m x n).\n",
        "    Y (numpy.ndarray): Target vector (m x 1).\n",
        "    W (numpy.ndarray): Initial guess for parameters (n x 1).\n",
        "    alpha (float): Learning rate.\n",
        "    iterations (int): Number of iterations for gradient descent.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing the final optimized parameters (W_update) and the history of cost values.\n",
        "    \"\"\"\n",
        "    # Initialize cost history\n",
        "    cost_history = [0] * iterations\n",
        "\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    for iteration in range(iterations):\n",
        "        # Step 1: Hypothesis Values (Predicted values)\n",
        "        Y_pred = np.dot(X, W)\n",
        "\n",
        "        # Step 2: Loss (Difference between predicted and actual values)\n",
        "        loss = Y_pred - Y\n",
        "\n",
        "        # Step 3: Gradient Calculation\n",
        "        # dw is the gradient of the cost function with respect to W\n",
        "        dw = (1 / m) * np.dot(X.T, loss)\n",
        "\n",
        "        # Step 4: Updating Weights using Gradient Descent\n",
        "        W_update = W - alpha * dw\n",
        "\n",
        "        # Step 5: Calculate new cost value (MSE)\n",
        "        cost = cost_function(X, Y, W_update)\n",
        "        cost_history[iteration] = cost\n",
        "\n",
        "        # Update weights for the next iteration\n",
        "        W = W_update\n",
        "\n",
        "    return W_update, cost_history\n",
        "\n",
        "# Example feature matrix (X), target vector (Y), and initial weight vector (W)\n",
        "X = np.array([[1, 2], [3, 4], [5, 6]])  # 3 samples, 2 features\n",
        "Y = np.array([[5], [11], [17]])  # 3 target values\n",
        "W = np.array([[0.5], [1]])  # Initial weights for the 2 features\n",
        "\n",
        "# Set learning rate and number of iterations\n",
        "alpha = 0.01\n",
        "iterations = 1000\n",
        "\n",
        "# Run gradient descent to optimize weights\n",
        "W_optimized, cost_history = gradient_descent(X, Y, W, alpha, iterations)\n",
        "\n",
        "# Output optimized weights and cost history\n",
        "print(\"Optimized Weights:\", W_optimized)\n",
        "print(\"Cost History (last 10 iterations):\", cost_history[-10:])  # Show last 10 cost values to observe convergence\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSGuwMKdPubO",
        "outputId": "f8fb77b5-7aea-49a6-d724-18e8386e4029"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimized Weights: [[1.07380851]\n",
            " [1.94173238]]\n",
            "Cost History (last 10 iterations): [0.00039606690591906853, 0.0003953688023979345, 0.00039467192934700157, 0.000393976284597462, 0.00039328186598430097, 0.00039258867134638953, 0.0003918966985263116, 0.00039120594537053613, 0.00039051640972927637, 0.0003898280894565749]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for RMSE:"
      ],
      "metadata": {
        "id": "MUo4Ps5yQ5fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the RMSE function\n",
        "def rmse(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This function calculates the Root Mean Squared Error (RMSE).\n",
        "    Parameters:\n",
        "    Y (numpy.ndarray): Array of actual (target) dependent variables.\n",
        "    Y_pred (numpy.ndarray): Array of predicted dependent variables.\n",
        "\n",
        "    Returns:\n",
        "    float: The root mean squared error.\n",
        "    \"\"\"\n",
        "    # Number of samples\n",
        "    m = len(Y)\n",
        "\n",
        "    # Calculate the squared differences between actual and predicted values\n",
        "    squared_diff = (Y - Y_pred) ** 2\n",
        "\n",
        "    # Calculate the mean squared error\n",
        "    mse = np.mean(squared_diff)\n",
        "\n",
        "    # Return the square root of the mean squared error (RMSE)\n",
        "    rmse_value = np.sqrt(mse)\n",
        "\n",
        "    return rmse_value\n",
        "\n",
        "# Example actual values and predicted values\n",
        "Y = np.array([[3], [10], [15]])\n",
        "Y_pred = np.array([[2.8], [10.5], [18.9]])\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse_value = rmse(Y, Y_pred)\n",
        "\n",
        "# Output the RMSE result\n",
        "print(\"RMSE:\", rmse_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nl2r6XlgQ_bc",
        "outputId": "800ff66a-5184-49bb-bad6-0ba7e7272b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RMSE: 2.2730302828309754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code for R-Squared Error:"
      ],
      "metadata": {
        "id": "N9ts6mlGRvNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the R² function\n",
        "def r2(Y, Y_pred):\n",
        "    \"\"\"\n",
        "    This function calculates the R Squared (R²) value.\n",
        "    Parameters:\n",
        "    Y (numpy.ndarray): Array of actual (target) dependent variables.\n",
        "    Y_pred (numpy.ndarray): Array of predicted dependent variables.\n",
        "\n",
        "    Returns:\n",
        "    float: The R squared value.\n",
        "    \"\"\"\n",
        "    # Mean of actual values\n",
        "    mean_y = np.mean(Y)\n",
        "\n",
        "    # Total sum of squares (ss_tot)\n",
        "    ss_tot = np.sum((Y - mean_y) ** 2)\n",
        "\n",
        "    # Residual sum of squares (ss_res)\n",
        "    ss_res = np.sum((Y - Y_pred) ** 2)\n",
        "\n",
        "    # R-squared value\n",
        "    r2 = 1 - (ss_res / ss_tot)\n",
        "\n",
        "    return r2\n",
        "\n",
        "# Example actual values and predicted values\n",
        "Y = np.array([[1], [7], [14]])\n",
        "Y_pred = np.array([[8.2], [12.4], [13.7]])\n",
        "\n",
        "# Calculate R²\n",
        "r2_value = r2(Y, Y_pred)\n",
        "\n",
        "# Output the R² result\n",
        "print(\"R²:\", r2_value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DScgKHecRz39",
        "outputId": "fb4c29ff-cdbc-4149-a13e-e964b31805d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R²: 0.042244094488189\n"
          ]
        }
      ]
    }
  ]
}